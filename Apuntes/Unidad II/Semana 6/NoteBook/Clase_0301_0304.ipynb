{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUYmBfyGq0qn"
   },
   "source": [
    "\n",
    "___\n",
    "___\n",
    "\n",
    "<!--Titulo -->\n",
    "## Clase 0301 Introducción al Entorno de Trabajo - 04/07/2021\n",
    "___\n",
    "\n",
    "* Instalar dependencias: \n",
    "  1. sudo apt install python3-pip\n",
    "    * Gestor de paquetes\n",
    "  2. sudo pip3 install --upgrade pip\n",
    "    * Actualizar a la última versión del gestor de paquetes\n",
    "  3. sudo pip3 install jupyter\n",
    "    * Editar código en py junto a markdown para documentar\n",
    "  4. sudo pip3 install numpy\n",
    "    * Realizar operaciones con arreglos de forma muy eficiente, dado que **numpy** está hecho en C y ya está compilado. Permite desarrollar algoritmos eficientes\n",
    "  5. sudo pip3 install matplotlib\n",
    "    * Sirve para gráficar o para mostrar gráficos\n",
    "  6. sudo pip3 install h5py\n",
    "    * Sirve para mostrar gráficos pero utilizando los dataset de los laboratorios ya elaborados en formato \"h5\"\n",
    "  7. sudo pip3 install scipy\n",
    "    * Métodos Cientificos\n",
    "  8. sudo pip3 install sklearn\n",
    "    * Importante en el área del aprendizaje autómatico, ya posee modelos creados para realizar el aprendizaje autómatico. Para comparar con los modelos creados\n",
    "  9. sudo pip3 install pillow\n",
    "    * Mostrar imagenes\n",
    "\n",
    "* Los Notebook de Jupyter se componen de celdas\n",
    "  1. Celdas de lenguaje de programación (Py)\n",
    "  2. Celdas de lenguaje de marcado (Markdown)\n",
    "\n",
    "* En Markdown se puede agregar código de Py utilizando el apostofre \n",
    "\n",
    "```python\n",
    "  def paraquememires(self):\n",
    "    print(\"Tengo color\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXohP1Oh6gRh"
   },
   "source": [
    "___\n",
    "<!--Titulo -->\n",
    "## 0302 Introducción a Numpy Lab 01 - 04/07\n",
    "___\n",
    "\n",
    "# Conceptos básicos de Python con Numpy (asignación opcional)\n",
    "\n",
    "Bienvenido a tu primera tarea. Este ejercicio le brinda una breve introducción a Python. Incluso si ha usado Python antes, esto lo ayudará a familiarizarse con las funciones que necesitaremos.\n",
    "\n",
    "**Instrucciones:**\n",
    "- Usarás Python 3.\n",
    "- Evite usar bucles for y while, a menos que se le indique explícitamente que lo haga.\n",
    "- No modifique el comentario (# FUNCIÓN CALIFICADA [nombre de la función]) en algunas celdas. Su trabajo no será calificado si cambia esto. Cada celda que contiene ese comentario solo debe contener una función.\n",
    "- Después de codificar su función, ejecute la celda justo debajo para verificar si su resultado es correcto.\n",
    "\n",
    "**Después de esta tarea:**\n",
    "- Poder utilizar iPython Notebooks\n",
    "- Ser capaz de usar funciones numpy y operaciones de matriz / vector numpy\n",
    "- Comprender el concepto de \"radiodifusión\"\n",
    "- Poder vectorizar código\n",
    "\n",
    "¡Empecemos!\n",
    "\n",
    "## Acerca de iPython Notebooks\n",
    "Los iPython Notebooks son entornos de codificación interactivos integrados en una página web. Utilizará cuadernos iPython en esta clase. Solo necesita escribir código entre:\n",
    "1. Los comentarios ### START CODE AQUÍ ### CODIGO AQUÍ ### END CODE AQUÍ ###. \n",
    "\n",
    "Después de escribir su código, puede ejecutar la celda presionando \"SHIFT\" + \"ENTER\" o haciendo clic en \"Ejecutar celda\" (indicado por un símbolo de reproducción) en la barra superior del cuaderno.\n",
    "\n",
    "A menudo especificaremos \"(≈ X líneas de código)\" en los comentarios para indicarle cuánto código necesita escribir. Es solo una estimación aproximada, así que no se sienta mal si su código es más largo o más corto.\n",
    "\n",
    "Ejercicio: establezca la prueba en \"Hola mundo\" en la celda siguiente para imprimir \"Hola mundo\" y ejecute las dos celdas siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCQvmOiuZvrb",
    "outputId": "26c7c572-e9d3-4ba0-9235-131ef24c90cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Test\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "test = 'Test'\n",
    "### END CODE HERE ###\n",
    "print (\"test: \" + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fdc3wIWQZ3q1"
   },
   "source": [
    "**Salida esperada**:\n",
    "test: Test\n",
    "___\n",
    "\n",
    "<h4 style=\"font:bold\">Lo que necesitas recordar:</h4>\n",
    "1. Ejecute sus celdas usando SHIFT + ENTER (o \"Ejecutar celda\") 2. Escriba código en las áreas designadas usando solo Python 3\n",
    "3. No modifique el código fuera de las áreas designadas\n",
    "\n",
    "___\n",
    "\n",
    "## 1 - Construyendo funciones básicas con numpy ##\n",
    "\n",
    "**Numpy es el paquete principal para la computación científica en Python**. Lo mantiene una gran comunidad (www.numpy.org). En este ejercicio aprenderá varias funciones clave como **np.exp, np.log y np.reshape**. Necesitará saber cómo utilizar estas funciones para futuras asignaciones.\n",
    "\n",
    "### 1.1 - función sigmoidea, np.exp () ###\n",
    "\n",
    "Antes de usar **np.exp (), usará math.exp ()** para implementar la función sigmoidea. Entonces verá por qué np.exp () es preferible a math.exp ().\n",
    "\n",
    "**Ejercicio**: Construye una función que devuelva el sigmoide de un número real x. Utilice math.exp (x) para la función exponencial.\n",
    "\n",
    "**Recordatorio**:\n",
    "$ sigmoide (x) = \\ frac {1} {1 + e ^ {- x}} $ a veces también se conoce como **función logística**. Es una función no lineal que se utiliza no solo en **Machine Learning** (Regresión logística), sino también en **Deep Learning**.\n",
    "\n",
    "![A vectorizado](https://drive.google.com/uc?export=view&id=1xMM52FfvYd8ztVcBhsMChfMGUxEy-q3S)\n",
    "\n",
    "\n",
    "Para hacer referencia a una función que pertenece a un paquete específico, puede llamarlo usando package_name.function (). Ejecute el siguiente código para ver un ejemplo con math.exp ().\n",
    "___\n",
    "## Anotaciones - Función logistica o función sigmoide \n",
    "* La libreria math tiene una función llamada exp que permite elevar numeros a un exponencial\n",
    "* Implementación con math.exp()\n",
    "    - s = 1.0 / (1.0 + math.exp(-x)) \n",
    "* La desventaja es que math.exp() esta diseñada para trabajar con números reales y en las redes neuronales se utilizan listas\n",
    "* La principal ventaja de **numpy** es poder transformar cualquier lista a un arreglo \n",
    "    - np.array([1,2,3])\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQVYW0JevcF0",
    "outputId": "9da50e7c-14c1-489b-ed63-9155248307b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRADED FUNCTION: basic_sigmoid\n",
    "\n",
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1.0 / (1.0 + math.exp(-x)) # Función sigmoide o logistica\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s\n",
    "\n",
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rLJ29OCvl2N"
   },
   "source": [
    "**Salida esperada**: \n",
    "<table style = \"width:40%\">\n",
    "    <tr>\n",
    "    <td>basic_sigmoid(3)</td> \n",
    "        <td>0.9525741268224334 </td> \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "____\n",
    "\n",
    "En realidad, rara vez usamos la biblioteca \"matemática\" en el aprendizaje profundo porque las entradas de las funciones son números reales. **En el aprendizaje profundo, usamos principalmente matrices y vectores. Por eso numpy es más útil.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "5TH-uVNxwVYm",
    "outputId": "6554605f-1181-41a9-8525-335c9b3bcc52"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8ccefa5bf989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbasic_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# you will see this give an error when you run it, because x is a vector.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-4119c81a13ab>\u001b[0m in \u001b[0;36mbasic_sigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m### START CODE HERE ### (≈ 1 line of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Función sigmoide o logistica\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n",
    "x = [1, 2, 3]\n",
    "basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njWff7dCwb-w"
   },
   "source": [
    "De hecho, si $ x = (x_1, x_2, ..., x_n) $ es un vector de fila, entonces $ np.exp (x) $ aplicará la función exponencial a cada elemento de x. La salida será entonces: $ np.exp (x) = (e ^ {x_1}, e ^ {x_2}, ..., e ^ {x_n}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avcEHm3JwjUs",
    "outputId": "7fab7ac8-58de-4005-d92c-e374136abf59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example of np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sNYyU_L_Jjf"
   },
   "source": [
    "Además, si x es un vector, entonces una operación de Python como $ s = x + 3 $ o $ s =  {1} / {x} $ generará s como un vector del mismo tamaño que x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSPN8wym_eZk",
    "outputId": "5c9ac5dd-bad5-4710-8f0c-32e1b1689add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# example of vector operation\n",
    "x = np.array([1, 2, 3])\n",
    "print (x + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "oVNOIiij_iZc",
    "outputId": "9c1891f4-06c1-44e9-c758-e83f0da37b6d"
   },
   "outputs": [],
   "source": [
    "np.exp?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVLmd7pG_f55"
   },
   "source": [
    "____\n",
    "### Anotaciones - numpy\n",
    "* Sobrecarga de los operadores básicos, es decir, crea versiones alternas que son capaces de funcionar con arreglos de np\n",
    "* Para ver la documentación de np se puede utilizar el signo de interrogación\n",
    "    - np.exp?\n",
    "____\n",
    "Siempre que necesite más información sobre una función numpy, le recomendamos que consulte [la documentación oficial] (https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html ).\n",
    "\n",
    "También puede crear una nueva celda en el cuaderno y escribir `np.exp?` (Por ejemplo) para obtener acceso rápido a la documentación.\n",
    "\n",
    "**Ejercicio**: Implemente la función sigmoidea usando numpy.\n",
    "\n",
    "**Instrucciones**: x ahora podría ser un número real, un vector o una matriz. Las estructuras de datos que usamos en numpy para representar estas formas (vectores, matrices ...) se denominan matrices numpy. No necesitas saber más por ahora.\n",
    "\n",
    "![Sigmoide](https://drive.google.com/uc?export=view&id=1dmauUn32o7zV9GkX7Q5M5ZlskRcIBjO-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVroiCQ7J5Dy",
    "outputId": "e34d34ae-56ac-4168-e2b6-6f5f446ad9ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "import numpy as np # this means you can access numpy functions by writing np.function() instead of numpy.function()\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1.0 / (1.0 + np.exp(-x)) # función sigmoide o logistica con np\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyBet6FnKB0n"
   },
   "source": [
    "**Salida esperada**: \n",
    "<table>\n",
    "    <tr> \n",
    "        <td> sigmoid([1,2,3])</td> \n",
    "        <td> array([ 0.73105858,  0.88079708,  0.95257413]) </td> \n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "_____\n",
    "\n",
    "### 1.2 - Gradiente sigmoide\n",
    "\n",
    "Como ha visto en la conferencia, deberá **calcular gradientes para optimizar las funciones de pérdida** mediante la **propagación inversa**. Codifiquemos su primera función de gradiente.\n",
    "\n",
    "**Ejercicio**: Implemente la función sigmoid_grad () para calcular el gradiente de la función sigmoidea con respecto a su entrada x. La fórmula es: $$ sigmoide \\ _derivative (x) = \\ sigma '(x) = \\ sigma (x) (1 - \\ sigma (x)) \\ tag {2} $$\n",
    "A menudo, codifica esta función en dos pasos:\n",
    "1. Establezca s para que sea el sigmoide de x. Puede que encuentre útil su función sigmoidea (x).\n",
    "2. Calcule $ \\ sigma '(x) = s (1-s) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5Biovf7KqJJ",
    "outputId": "28d65771-6804-46f9-de89-1176b5c1079c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    s = sigmoid(x) \n",
    "    ds = s * (1.0 - s) # Gradiente de la función sigmoide o logistica con el mismo tamaño de s\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return ds\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGZ9pIwPKB6A"
   },
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "        <td>sigmoid_derivative([1,2,3])</td> \n",
    "        <td> [ 0.19661193  0.10499359  0.04517666] </td> \n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "____\n",
    "### Anotaciones - Shape y reshape\n",
    "* Permiten conocer las dimansiones y tamaños de las dimensiones\n",
    "    1. forma -> Conocer dimensiones\n",
    "    2. remodelar -> Cambiar dimensiones y tamaños\n",
    "        * Hace sentido usar reshape cuando la dimensión a la que se está convirtiendo tiene la capacidad de albergar de todos los elementos de la dimensión anterior\n",
    "* Ejemplo\n",
    "    1. Cada píxel contiene la información de 3 niveles de color que se representan con 3 vectores\n",
    "    2. Para convertir el arreglo de 3 dimensiones de los colores a un vector de una sola dimensión se ** reshape **\n",
    "    3. Al hacer un ** reshape ** de los 3 vectores a 1 solo vector, este vector ahora de una dimensión sirve como un vector de ** atributos **\n",
    "____\n",
    "    \n",
    "### 1.3 - Remodelando matrices ###\n",
    "\n",
    "Dos funciones comunes que se utilizan en el aprendizaje profundo son [np.shape] (https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) y [np.reshape ()] ( https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html).\n",
    "- X.shape se usa para obtener la forma (dimensión) de una matriz / vector X.\n",
    "- X.reshape (...) se usa para remodelar X en alguna otra dimensión.\n",
    "\n",
    "Por ejemplo, en informática, una imagen se representa mediante una matriz 3D de forma $ (longitud, altura, profundidad = 3) $. Sin embargo, cuando lee una imagen como entrada de un algoritmo, la convierte en un vector de forma $ (longitud * altura * 3, 1) $. En otras palabras, \"desenrolla\", o remodela, la matriz 3D en un vector 1D.\n",
    "\n",
    "![Sigmoide](https://drive.google.com/uc?export=view&id=1dmauUn32o7zV9GkX7Q5M5ZlskRcIBjO-)\n",
    "\n",
    "\n",
    "**Ejercicio**: \n",
    "  * Implemente `image2vector ()` que toma una entrada de forma (longitud, altura, 3) y devuelve un vector de forma (longitud \\ * altura \\ * 3, 1). Por ejemplo, si desea remodelar una matriz v de forma (a, b, c) en un vector de forma (a * b, c), debería hacer:\n",
    "\n",
    "```\n",
    "v = v.reshape ((v.shape [0] * v.shape [1], v.shape [2])) \n",
    "# v.shape [0] = a; v.shape [1] = b; v.shape [2] = c\n",
    "```\n",
    "\n",
    "- No codifique las dimensiones de la imagen como una constante. En su lugar, busque las cantidades que necesita con `image.shape [0]`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojZP_0BjPNFW",
    "outputId": "dc902f8f-f39e-4b73-be77-5e25adacae4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: image2vector\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2],1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v\n",
    "\n",
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSsdK1v-Pj3p"
   },
   "source": [
    "### 1.4 - Normalización de filas\n",
    "\n",
    "Otra técnica común que usamos en Machine Learning y Deep Learning es normalizar nuestros datos. A menudo conduce a un mejor rendimiento porque el descenso de gradientes converge más rápido después de la normalización. Aquí, por normalización nos referimos a cambiar x por $ \\ frac {x} {\\ | x \\ |} $ (dividiendo cada vector fila de x por su norma).\n",
    "\n",
    "Por ejemplo, si $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ entonces $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$y        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$  Tenga en cuenta que puede dividir matrices de diferentes tamaños y funciona bien: esto se llama transmisión y lo aprenderá en la parte 5.\n",
    "\n",
    "\n",
    "**Ejercicio**: Implemente normalizeRows () para normalizar las filas de una matriz. Después de aplicar esta función a una matriz de entrada x, cada fila de x debe ser un vector de longitud unitaria (es decir, longitud 1).\n",
    "\n",
    "\n",
    "### Anotaciones - Normalizar datos\n",
    "* Es útil para preparar los datos previo a la ejecución de un algoritmo de aprendizaje de maquina\n",
    "* Sirve para que los diferentes atributos se encuentren en la misma escala, lo que permite que el algoritmo de descenso de gradiente converja más rápido, es decir, encuentra su camino al punto minímo de forma más rápida\n",
    "* Para normalizar las filas de un arreglo np\n",
    "    1. Considerar cada una de las filas de una matriz como un vector\n",
    "    2. Dividir cada uno de los elementos de las filas de los vectores por la norma del vector (conmunmente se utiliza la norma **euclidia** )\n",
    "    3. **Norma euclidia:** Se calcula elevando los elementos al cuadrado, sumandolos y sacando la raíz cuadrada\n",
    "    4. Se hace uso de la función **np.linarg.norm** la cual tiene como parámetros\n",
    "        1. Matriz de donde se obtendran las filas como vectores\n",
    "        2. ord = 2\n",
    "            * Indicar que se hace uso de la norma euclidia\n",
    "        3. axis = 1 | axis = 0 \n",
    "            * 1 --> se **mueve** a nivel de **columnas**, **calcula la norma** a nivel de las **filas**\n",
    "            * 0 --> se **mueve** a nivel de **filas**, **calcula la norma** a nivel de las **columnas**\n",
    "        4. keepdims = True | keepdims = False\n",
    "            * Sirve para que el vector resultante mantenga las dimensiones que el vector original, es decir, el **vector resultante es un vector columna** el cual tiene la capacidad de operar con vectores con dimensiones iguales a las del vector original\n",
    "            * Siempre se debe mandar en True para que se pueda diferenciar entre vectores columna y vectores fila\n",
    "    5. Dividir la matriz entre la norma de la matriz  \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dl0lo3IYuOsd",
    "outputId": "ffe54051-3d40-4212-da64-fd84a0e3b835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: normalizeRows\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    \n",
    "    x_norm = np.linalg.norm(x, ord=2, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Dividir x por la norma \n",
    "    x = x / x_norm # Gracias al broadcasting esto se puede operar\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return x\n",
    "\n",
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmHGP6LwvqKr"
   },
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:60%\">\n",
    "     <tr> \n",
    "       <td> normalizeRows(x) </td> \n",
    "       <td> [[ 0.          0.6         0.8       ]\n",
    "[ 0.13736056  0.82416338  0.54944226]]</td> \n",
    "     </tr>\n",
    "</table>\n",
    "\n",
    "___\n",
    "\n",
    "**Nota**:\n",
    "En normalizeRows (), puede intentar imprimir las formas de x_norm y x, y luego volver a ejecutar la evaluación. Descubrirás que tienen diferentes formas. Esto es normal dado que x_norm toma la norma de cada fila de x. Entonces **x_norm tiene el mismo número de filas pero solo 1 columna**. Entonces, ¿cómo funcionó cuando dividió x por x_norm? ¡Esto se llama **transmisión o broadcasting** y hablaremos de eso ahora!\n",
    "\n",
    "____\n",
    "### Anotaciones - Broadcasting y función softmax\n",
    "#### Función Softmax\n",
    "* Se utiliza para hacer clasificación multiclase\n",
    "* Funciona sobre un vector (puede ser vector fila) o matriz  y lo que realiza es:\n",
    "    1. Elevar e a cada uno de los elementos del vector enviado como parámetro\n",
    "    2. Dividr entre la sumatoria de todos los elementos del vector enviado como parámetro\n",
    "* Se parace a la norma euclidea, con la diferencia de que ahora se divide por la sumatoria de cada uno de los elementos \n",
    "* Si el parámetro es una matriz, considera que cada fila de la matriz es un vector fila y aplica el procedimiento correspondiente\n",
    "____\n",
    "### 1.5 - Difusión o Broadcasting y función softmax ####\n",
    "Un concepto muy importante de entender en **numpy es \"radiodifusión\"**. Es muy útil para realizar **operaciones matemáticas entre matrices de diferentes formas**. Para obtener todos los detalles sobre la transmisión, puede leer la [documentación de transmisión] oficial (http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
    "\n",
    "** Ejercicio **: Implemente una función softmax usando numpy. Puede pensar en softmax como una función de normalización utilizada cuando su algoritmo necesita clasificar dos o más clases. Aprenderá más sobre softmax en el segundo curso de esta especialización.\n",
    "\n",
    "**Instrucciones**:- $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  $$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQYEkN0b2ly3",
    "outputId": "10680dc1-cfff-4e9e-bb09-032429fa68b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (n, m).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s = x_exp / x_sum\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s\n",
    "\n",
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmUt0kTO2w4u"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "     <tr> \n",
    "       <td>softmax(x)</td> \n",
    "       <td> [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04 1.21052389e-04][  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04 8.01252314e-04]]</td> \n",
    "     </tr>\n",
    "</table>\n",
    "\n",
    "____\n",
    "\n",
    "**Nota**:\n",
    "- Si imprime las formas de x_exp, x_sum y s arriba y vuelve a ejecutar la celda de evaluación, verá que x_sum tiene forma (2,1) mientras que x_exp ys tienen forma (2,5). **x_exp / x_sum** funciona debido al **broadcasting de Python**.\n",
    "\n",
    "¡Felicidades! Ahora tiene una comprensión bastante buena de Python Numpy y ha implementado algunas funciones útiles que utilizará en el aprendizaje profundo.\n",
    "\n",
    "____\n",
    "## Lo que necesita recordar:\n",
    "- np.exp(x) funciona para cualquier np.array x y aplica la función exponencial a cada coordenada\n",
    "- La función sigmoidea y su gradiente\n",
    "- Iage2vector se usa comúnmente en aprendizaje profundo\n",
    "- np.reshape se usa ampliamente. En el futuro, verá que mantener las dimensiones de su matriz / vector correctas ayudará a eliminar muchos errores.\n",
    "- numpy tiene funciones integradas eficientes\n",
    "- la transmisión es extremadamente útil\n",
    "____\n",
    "## 2) Vectorización\n",
    "\n",
    "En el aprendizaje profundo, se trabaja con conjuntos de datos muy grandes. Por lo tanto, una función no óptima desde el punto de vista computacional puede convertirse en un gran cuello de botella en su algoritmo y puede resultar en un modelo que tarda años en ejecutarse. Para asegurarse de que su código sea computacionalmente eficiente, utilizará la vectorización. Por ejemplo, intente diferenciar entre las siguientes implementaciones del producto punto / exterior / elemento.\n",
    "____\n",
    "### Anotaciones\n",
    "* Permite ejecutar redes con grandes tamaños de Data set\n",
    "* Permite que los algoritmos se ejecuten de la forma más optima aprovechando las capacidad de procesamiento parálelo que tienen los procesadores actuales\n",
    "* Utilizando np es la forma más sencilla de vectorizar, porque las funciones de np ya tienen vectorización internamente, es decir son capaces de procesar de manera paralela los vectores\n",
    "* Hay que limitar la cantidad de ciclos o iteraciones (como ciclos for) que limitan la ejecución en paralelo y remplazarlas por funciones de np vectorizadas\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3bcRCU-QiUc",
    "outputId": "70fb4ce2-b465-4d5f-c4cc-909312f031bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 2534.939860904167\n",
      " ----- Computation time = 5.694289000000019ms\n",
      "outer = [[3.58435629e-01 4.44478985e-02 2.70398802e-01 ... 4.29578843e-01\n",
      "  5.50867222e-01 5.61690072e-01]\n",
      " [2.89746417e-01 3.59300759e-02 2.18580627e-01 ... 3.47256023e-01\n",
      "  4.45301168e-01 4.54049969e-01]\n",
      " [3.13983786e-03 3.89356368e-04 2.36864957e-03 ... 3.76304087e-03\n",
      "  4.82550735e-03 4.92031376e-03]\n",
      " ...\n",
      " [3.64372275e-01 4.51840739e-02 2.74877325e-01 ... 4.36693811e-01\n",
      "  5.59991047e-01 5.70993153e-01]\n",
      " [2.09634183e-01 2.59957386e-02 1.58145083e-01 ... 2.51242909e-01\n",
      "  3.22179468e-01 3.28509306e-01]\n",
      " [2.52113643e-01 3.12634145e-02 1.90190991e-01 ... 3.02153800e-01\n",
      "  3.87464669e-01 3.95077161e-01]]\n",
      " ----- Computation time = 35728.412262ms\n",
      "elementwise multiplication = [0.35843563 0.03593008 0.00236865 ... 0.43669381 0.32217947 0.39507716]\n",
      " ----- Computation time = 3.138661999997794ms\n",
      "gdot = [2486.3454678  2485.2166619  2518.32003444]\n",
      " ----- Computation time = 12.86246999999463ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "#x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "#x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "x1 = np.random.rand(10000)\n",
    "x2 = np.random.rand(10000)\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time() # Medir tiempo al inicio en Milisegundos\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i] # Producto punto\n",
    "toc = time.process_time() # Medir tiempo al final \n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\") # Tiempo de ejecución en Milisegundos\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)): # 1er ciclo for\n",
    "    for j in range(len(x2)): # 2do ciclo for anidado\n",
    "        outer[i,j] = x1[i]*x2[j] # Producto externo: multiplica un elemento por todo el resto de elementos \n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)): \n",
    "    mul[i] = x1[i]*x2[i] # Multiplicación de elemento x elemento y se almacenan en un vector\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]): # 1er ciclo for\n",
    "    for j in range(len(x1)):  # 2do ciclo for anidado\n",
    "        gdot[i] += W[i,j]*x1[j] # Multiplicación una matriz x un vector\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpzHKuW3QoY1",
    "outputId": "ed031ccd-7621-4959-f938-8543f069b067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 2490.75455236482\n",
      " ----- Computation time = 0.24590400000334967ms\n",
      "outer = [[0.1367482  0.10643442 0.2296066  ... 0.09083267 0.07628295 0.21175142]\n",
      " [0.20215658 0.15734334 0.33943031 ... 0.13427908 0.11277004 0.31303478]\n",
      " [0.32261813 0.25110147 0.54169086 ... 0.21429362 0.17996772 0.49956669]\n",
      " ...\n",
      " [0.49401844 0.38450646 0.82947996 ... 0.32814336 0.27558083 0.7649761 ]\n",
      " [0.30321673 0.23600089 0.509115   ... 0.20140656 0.16914494 0.46952407]\n",
      " [0.03673751 0.02859369 0.06168398 ... 0.02440226 0.02049347 0.05688718]]\n",
      " ----- Computation time = 133.71959300000213ms\n",
      "elementwise multiplication = [0.1367482  0.15734334 0.54169086 ... 0.32814336 0.16914494 0.05688718]\n",
      " ----- Computation time = 0.12054899999469626ms\n",
      "gdot = [2484.4564902  2465.56405713 2524.91672798]\n",
      " ----- Computation time = 0.3679300000030139ms\n"
     ]
    }
   ],
   "source": [
    "#x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "#x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "x1 = np.random.rand(10000)\n",
    "x2 = np.random.rand(10000)\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2) # Calcula el producto punto con np\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2) # Calcula el producto externo con np\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2) # Calcula multiplicación elemento a elemento con np => np.array(x1) * np.array(x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1) # # Multiplicación una matriz x un vector con np\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCeq7KERTsi3"
   },
   "source": [
    "Como habrás notado, la implementación vectorizada es mucho más limpia y eficiente. Para vectores / matrices más grandes, las diferencias en el tiempo de ejecución se vuelven aún mayores.\n",
    "\n",
    "**Tenga en cuenta** que `np.dot ()` realiza una multiplicación matriz-matriz o matriz-vector. Esto es diferente de `np.multiply ()` y el operador `*` (que es equivalente a `. *` En Matlab / Octave), que realiza una multiplicación por elementos.\n",
    "\n",
    "____\n",
    "### Anotaciones - Funciones de perdida L1 y L2\n",
    "* Sirven para comparar etiquetas reales contra etiquetas predecidas\n",
    "____\n",
    "\n",
    "### 2.1 Implementar las funciones de pérdida L1 y L2\n",
    "\n",
    "**Ejercicio**: \n",
    "Implemente la versión numpy vectorizada de la pérdida L1. Puede encontrar útil la función abs (x) (valor absoluto de x).\n",
    "\n",
    "**Recordatorio**:\n",
    "- La **pérdida se utiliza para evaluar el rendimiento de su modelo**. Cuanto mayor sea su pérdida, más diferentes serán sus predicciones ($ \\ hat ({y}) $) de los valores reales ($ y $). En el aprendizaje profundo, se utilizam algoritmos de optimización como **Gradient Descent** para entrenar su modelo y **minimizar el costo**.\n",
    "- La **pérdida L1** se define como:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dR7baIJVG0F",
    "outputId": "ecf2047e-687c-4895-ae57-871e754d5561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = [1.1]\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: L1\n",
    "\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = np.sum(abs(y-yhat), keepdims = True)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cncWEy1EKLno"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> L1 </td> \n",
    "       <td> 1.1 </td> \n",
    "     </tr>\n",
    "</table>\n",
    "\n",
    "____\n",
    "**Ejercicio**: Implemente la versión numpy vectorizada de la pérdida L2. Hay varias formas de implementar la pérdida L2, pero la función np.dot () puede resultarle útil. Como recordatorio, si $x = [x_1, x_2, ..., x_n]$, then `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "- La perdida de L2 se define como $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFHUmHUkKjid",
    "outputId": "0a44176b-1344-4335-d2ca-5ed7c3a3f193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = [0.43]\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: L2\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = np.sum(np.square(abs(y-yhat)), keepdims = True)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-ItIws0Lh9f"
   },
   "source": [
    "**Expected Output**: \n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **L2** </td> \n",
    "       <td> 0.43 </td> \n",
    "     </tr>\n",
    "</table>\n",
    "\n",
    "____\n",
    "\n",
    "Felicitaciones por completar esta tarea. Esperamos que este pequeño ejercicio de calentamiento te ayude en las asignaciones futuras, ¡que serán más emocionantes e interesantes!\n",
    "\n",
    "_____\n",
    "**Qué recordar:**\n",
    "- La vectorización es muy importante en el aprendizaje profundo. Proporciona eficiencia y claridad computacionales.\n",
    "- Ha revisado la pérdida L1 y L2.\n",
    "- Está familiarizado con muchas funciones numpy como np.sum, np.dot, np.multiply, np.maximum, etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCoRvSUIjkhu"
   },
   "source": [
    "___\n",
    "## 0303 Vectorización de la Regresión Logística - 05/07\n",
    "___\n",
    "### Vectorización\n",
    "* Permite implementar los algoritmos de forma eficiente\n",
    "\n",
    "### Vectorización del algoritmo de regresión lineal\n",
    "\n",
    "#### Algoritmo sin vectorizar\n",
    "- Datos:\n",
    "    * Instancias de 1 a m \n",
    "    * Instancias solo con 2 atributos\n",
    "    * En “y” se almacenan las etiquetas\n",
    "    * En “J” se almacenará el costo\n",
    "    * En “dw1” se almacenará la derivada de J con respecto a w1\n",
    "```\n",
    "— Pseudocódigo — \n",
    "logistic_regression (x1[m], x2[m], y[m]):\n",
    "\n",
    "\t— Inicialización de variables — \n",
    "\tJ, dw1, dw2, db := 0;\n",
    "\tz, y_hat, dz := array[m];\n",
    "\n",
    "\tfor i = 1 to m\n",
    "\t\t— Prop. Hacía adelante — \n",
    "\t\t/*  w1 y w2 son los parametros del modelo, inicial teniendo un valor \t\taleatorio, el algo. De descenso de gradiente es el que irá ajustando \t\tlos valores de w1 y w2 a los valores reales */\n",
    "\t\tz[ i ] := w1*x1[ i ]  + w2*x2[ i ]  + b;\n",
    "\t\ty_hat[ i ] := sigmoid(z[ i ]);\n",
    "\t\tJ += L(y[ i ], y_hat[ i ]); // J acumula la suma de las perdidas\n",
    "        dz[i] := y_hat[i] - y[i]\n",
    "\t\t— Prop. Hacia atrás — \n",
    "\t\tdw1 += x1[ i ] * dz[ i ]; \n",
    "\t\tdw2 += x2[ i ] * dz[ i ]; \n",
    "\t\tdb += dz[ i ];\n",
    "\n",
    "\tJ /= m; // Obtener el costo\n",
    "\tdw1 /= m; // Obtener la derivada del costo con respecto a w1\n",
    "\tdw2 /= m; // Obtener la derivada del costo con respecto a w2\n",
    "\tdb /= m; // Obtener la derivada del costo con respecto a b\n",
    "```\n",
    "#### Vectorización de Propagación para adelante\n",
    "##### **Función Logistica**\n",
    "- Datos:    \n",
    "    * X --> es la matriz que contiene todas las instancias de entrenamiento de tamaño (n,m) cada instancia tiene n elementos o atributos\n",
    "    * w --> es Un vector columna\n",
    "    * wT --> es un vector fila\n",
    "    * n --> es el tamaño de w (Para cada atributo hay un w)\n",
    "    * Cuando se operan vectores se mantienen los tamaños de dimensiones tales que: \n",
    "        1. Se conserva la primer dimensión del primer operando \n",
    "        2. Se conserva la segunda dimensión del segundo operando\n",
    "        * Ejemplo: (1,n) prod.punto (n,m) => (1,m)\n",
    "- La vectorización en la propapagación hacía adelante comienza con la función logistica donde para desarrollar z se debe resolver el producto punto entre el vector fila y cada uno de los vectores columna que conforman la matriz X\n",
    "    * Gracias al broadcasting al momento de sumar **b** para completar la propagación hacía adelante, se logra sumar **b** a cada uno de los elementos que conforman la matriz\n",
    "    * **Z** hace referencia a la función de z pero vectorizada\n",
    "\n",
    "##### Pasos para vecotrizar ***z***\n",
    "![Z vectorizado](https://drive.google.com/uc?export=view&id=1T-3tZ4VHW7w4eN9cVOnWHnSkCBUf3Xvb)\n",
    "\n",
    "##### Pasos para vectorizar ***y_hat*** la cual es la función sigmoide de ***z***\n",
    "* Desarrollado previamente en el lab01\n",
    "    - Recibir un vector para implementarle la función sigmoide, retornando un vector, donde se habra realizado la función sigmoide a cada uno de los elementos del vector usando **numpy** especificamente con la función **np.exp()**\n",
    "* **A** hace referencia a la función logistica vectorizada, es decir, ***y_hat*** vectorizado\n",
    "\n",
    "![A vectorizado](https://drive.google.com/uc?export=view&id=11SNmXka877ZvV2ph6ot5120RCIQ1NpKZ)\n",
    "\n",
    "#### Vectorización de Propagación para atras\n",
    "1. Lo primero que se calcula en la propagación hacía atras es **dZ** para cada una de las **m** instancias definido como vector fila (1,m) \n",
    "    * dz[i] := y_hat[i] - y[i]\n",
    "    * **y_hat** o **A** ya se proporciona en la prop. hacía adelante y **y** se proporciona como parámetro\n",
    "\n",
    "![dZ vectorizado](https://drive.google.com/uc?export=view&id=10_HRuBa8TlE6lBTJMbsNpFhRN7TCU7pr)\n",
    "\n",
    "2. Calcular **dw**, el cual se debe calcular \"n\" veces, es decir si n=2 se debe calcular dw1 y dw2\n",
    "    * Si n es un número que no está definido se dice que se deben de tener **dw** n veces \n",
    "    * Componentes de dw:\n",
    "        * dw1 += x1[ i ] * dz[ i ]; \n",
    "\t\t* dw2 += x2[ i ] * dz[ i ];\n",
    "        * Al final se divide entre m \n",
    "            * dw1 /= m; \n",
    "\t        * dw2 /= m; \n",
    "\n",
    "##### Pasos para vectorizar **dw**\n",
    "1. Se propone un vector **dw** que contiene las n dw a calcular el cual es un vector columna\n",
    "2. **dw** Se define como:\n",
    "    * 1/m * La sumatoria de dw en cada uno de los n atributos\n",
    "\n",
    "![dw propuesto](https://drive.google.com/uc?export=view&id=1pngUzfrqraEx1z916dHoR6v3Ca3Zq2j6)\n",
    "\n",
    "3. La vecotorización de **dw** se define matemáticamente como:\n",
    "\n",
    "![dw propuesto](https://drive.google.com/uc?export=view&id=1KXxGcarOBsOC2g_PE2mdlVMNR21AlVQ3)\n",
    "\n",
    "4. La vecotorización de **dw** utilizando **numpy** no retorna un vector, sino un número real.\n",
    "5. La vecotorización de **dw** utilizando **numpy** queda de está manera:\n",
    "\n",
    "![dw propuesto](https://drive.google.com/uc?export=view&id=1omXSDq2cBrTX1GiVirHFhnsetl9PaTLf)\n",
    "\n",
    "### Resumen de formulas vistas\n",
    "![dw propuesto](https://drive.google.com/uc?export=view&id=1RLKeioMSqXss_BOmdUCqxKuKgWlqt5sJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd-XNKk5fWFX"
   },
   "source": [
    "___\n",
    "## 0303 Vectorización de la Regresión Logística - 05/07\n",
    "___\n",
    "# Regresión logística con una mentalidad de red neuronal\n",
    "\n",
    "¡Bienvenido a su primera asignación de programación (requerida)! Construirá un clasificador de regresión logística para reconocer gatos. Esta tarea lo guiará a través de cómo hacer esto con una mentalidad de red neuronal y, por lo tanto, también perfeccionará sus intuiciones sobre el aprendizaje profundo.\n",
    "\n",
    "**Instrucciones:**\n",
    "- No use bucles (for / while) en su código, a menos que las instrucciones le pidan explícitamente que lo haga.\n",
    "\n",
    "**Aprenderás a:**\n",
    "- Construir la arquitectura general de un algoritmo de aprendizaje, que incluye:\n",
    "     - Inicializando parámetros\n",
    "     - Cálculo de la función de coste y su gradiente.\n",
    "     - Usando un algoritmo de optimización (descenso de gradiente)\n",
    "- Reúna las tres funciones anteriores en una función de modelo principal, en el orden correcto.\n",
    "____\n",
    "### Anotaciones - Regresión logística en redes neuronales\n",
    "* **La red neuronal de este ejercicio solo posee 1 neurona**\n",
    "* La regresión logística es utilizada también en la estadística\n",
    "____\n",
    "## <font color = 'darkblue'> Actualizaciones </font>\n",
    "Este portátil se ha actualizado durante los últimos meses. La versión anterior se llamaba \"v5\" y la versión actual ahora se llama \"6a\".\n",
    "\n",
    "#### Si estaba trabajando en una versión anterior:\n",
    "* Puede encontrar su trabajo anterior buscando en el directorio de archivos los archivos más antiguos (nombrados por el nombre de la versión).\n",
    "* Para ver el directorio de archivos, haga clic en el icono \"Coursera\" en la esquina superior izquierda de este cuaderno.\n",
    "* Copie su trabajo de las versiones anteriores a la nueva para enviar su trabajo para su calificación.\n",
    "\n",
    "#### Lista de actualizaciones\n",
    "* Fórmula de propagación hacia adelante, la indexación ahora comienza en 1 en lugar de 0.\n",
    "* El comentario de la función de optimización ahora dice \"costo de impresión cada 100 iteraciones de entrenamiento\" en lugar de \"ejemplos\".\n",
    "* Gramática fija en los comentarios.\n",
    "* El nombre de la variable Y_prediction_test se utiliza de forma coherente.\n",
    "* La etiqueta del eje del gráfico ahora dice \"iteraciones (cien)\" en lugar de \"iteraciones\".\n",
    "* Al probar el modelo, la imagen de prueba se normaliza dividiendo por 255.\n",
    "\n",
    "## 1 - Paquetes ##\n",
    "\n",
    "Primero, ejecutemos la celda a continuación para importar todos los paquetes que necesitará durante esta asignación.\n",
    "- [numpy] (www.numpy.org) es el paquete fundamental para la computación científica con Python.\n",
    "- [h5py] (http://www.h5py.org) es un paquete común para interactuar con un conjunto de datos que se almacena en un archivo H5.\n",
    "- [matplotlib] (http://matplotlib.org) es una famosa biblioteca para trazar gráficos en Python.\n",
    "- [PIL] (http://www.pythonware.com/products/pil/) y [scipy] (https://www.scipy.org/) se utilizan aquí para probar su modelo con su propia imagen al final .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fl50Dk-IfsEQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # pyplot es el módulo que comunmente se utiliza\n",
    "import h5py # Leer data set\n",
    "import scipy # Mostrar imagenes propias y probar el modelo\n",
    "from PIL import Image # Mostrar imagenes propias\n",
    "from scipy import ndimage # Mostrar imagenes propias\n",
    "from lr_utils import load_dataset # Contiene una función para cargar dataset usando h5py\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75ZOaOzkftYJ"
   },
   "source": [
    "## 2 - Descripción general del conjunto de problemas ##\n",
    "\n",
    "**Declaración del problema**: se le proporciona un conjunto de datos (\"data.h5\") que contiene:\n",
    "\n",
    "    - un conjunto de entrenamiento de imágenes de m_train etiquetadas como gato (y = 1) o no gato (y = 0)\n",
    "    - un conjunto de prueba de imágenes m_test etiquetadas como gato o no gato\n",
    "    - cada imagen tiene la forma (num_px, num_px, 3) donde 3 es para los 3 canales (RGB). Por lo tanto, cada imagen es cuadrada (alto = num_px) y (ancho = num_px).\n",
    "\n",
    "Construirá un algoritmo de reconocimiento de imágenes simple que puede clasificar correctamente las imágenes como gatos o no gatos.\n",
    "\n",
    "Familiaricémonos más con el conjunto de datos. Cargue los datos ejecutando el siguiente código.\n",
    "____\n",
    "### Anotaciones - Análisis del problema\n",
    "0. **Preprocesado - Establecer las dimensiones a los vectores** \n",
    "1. Se tiene un conjunto de datos en formato h5 que contiene:\n",
    "    * Ambos estan etiquetados y = 1 -> Es gato | y = 0 -> No es gato\n",
    "    * Cada uno de las imágenes tiene las dimensiones -> (num_px, num_px, 3) los cuales debenran ser cambiados a arreglos o vectores columna\n",
    "        1. Conjunto de entraiemto\n",
    "\n",
    "        2. Conjunto de prueba -> Sirve para evaluar el modelo, contiene instancias, que son tomadas de la misma distribución del conjunto de entrenamiento\n",
    "\n",
    "* Es importante que el modelo sea capaz de generalizar, es decir, que también funcione con conjuntos de datos que no sean los mismos datos de entrenamiento\n",
    "\n",
    "2. Lo que devuelve el método load_dataset () es una tupla de 5 elementos\n",
    "    1. Atributos del conjunto de entrenamiento en formato original\n",
    "    2. Las etiquetas del conjunto de entrenamiento\n",
    "    3. Atributos del conjunto de entrenamiento en formato original\n",
    "    4. Las etiquetas del conjunto de entrenamiento\n",
    "    5. Nombres de las clases -> cat & noncat\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "deOAIK3pgZh2"
   },
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() # load_dataset devuelve una tupla con 5 elementos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYT4qcMlgc0s"
   },
   "source": [
    "\n",
    "Agregamos \"_orig\" al final de los conjuntos de datos de imágenes (entrenar y probar) porque los vamos a preprocesar. Después del preprocesamiento, terminaremos con train_set_x y test_set_x (las etiquetas train_set_y y test_set_y no necesitan ningún preprocesamiento).\n",
    "\n",
    "Cada línea de tu train_set_x_orig y test_set_x_orig es una matriz que representa una imagen. Puede visualizar un ejemplo ejecutando el siguiente código. Siéntase libre también de cambiar el valor del \"índice\" y volver a ejecutarlo para ver otras imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "L0K-0b29gecQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 24\n",
    "plt.imshow(train_set_x_orig[index]) \n",
    "# Mostrar imagen y etiqueta en el conjunto de entrenamiento\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0VnLDEjgyOl"
   },
   "source": [
    "\n",
    "![dZ vectorizado](https://drive.google.com/uc?export=view&id=1p0SN6ufNlrhhKRXt8PLDi6Fc4OPgqQKn)\n",
    "_____\n",
    "Muchos errores de software en el aprendizaje profundo provienen de tener dimensiones matriciales / vectoriales que no encajan. Si puede mantener las dimensiones de su matriz / vector rectas, avanzará en gran medida hacia la eliminación de muchos errores.\n",
    "\n",
    "**Ejercicio:** Encuentra los valores para:\n",
    "     - m_train (número de ejemplos de entrenamiento)\n",
    "     - m_test (número de ejemplos de prueba)\n",
    "     - num_px (= altura = ancho de una imagen de entrenamiento)\n",
    "Recuerda que `train_set_x_orig` es una matriz numérica de formas (m_train, num_px, num_px, 3). Por ejemplo, puede acceder a `m_train` escribiendo` train_set_x_orig.shape [0] `.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "51ebKCOWhBK6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n",
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n------------------------------ Resultado esperado ------------------------------\\nNumber of training examples: m_train = 209\\nNumber of testing examples: m_test = 50\\nHeight/Width of each image: num_px = 64\\nEach image is of size: (64, 64, 3)\\ntrain_set_x shape: (209, 64, 64, 3)\\ntrain_set_y shape: (1, 209)\\ntest_set_x shape: (50, 64, 64, 3)\\ntest_set_y shape: (1, 50)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ Resultado esperado ------------------------------\n",
    "\n",
    "train_set_x shape: (209, 64, 64, 3)\n",
    "train_set_y shape: (1, 209)\n",
    "test_set_x shape: (50, 64, 64, 3)\n",
    "test_set_y shape: (1, 50)\n",
    "\"\"\"\n",
    "################################################################################\n",
    "\n",
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "m_train = train_set_x_orig.shape[0] # Instancias del conjunto de entrenamiento \n",
    "m_test = test_set_x_orig.shape[0] # Instancias del conjunto de prueba \n",
    "num_px = test_set_x_orig.shape[1] # Dimensiones de las imagenes\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ Resultado esperado ------------------------------\n",
    "Number of training examples: m_train = 209\n",
    "Number of testing examples: m_test = 50\n",
    "Height/Width of each image: num_px = 64\n",
    "Each image is of size: (64, 64, 3)\n",
    "train_set_x shape: (209, 64, 64, 3)\n",
    "train_set_y shape: (1, 209)\n",
    "test_set_x shape: (50, 64, 64, 3)\n",
    "test_set_y shape: (1, 50)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CPl9Q1OhrJO"
   },
   "source": [
    "Para mayor comodidad, ahora debe remodelar las imágenes de forma (num_px, num_px, 3) en una matriz de formas numpy (num_px $ * $ num_px $ * $ 3, 1). Después de esto, nuestro conjunto de datos de entrenamiento (y prueba) es una matriz numérica donde cada columna representa una imagen plana. Debe haber columnas m_train (respectivamente m_test).\n",
    "\n",
    "**Ejercicio:** Reforma los conjuntos de datos de entrenamiento y prueba para que las imágenes de tamaño (num_px, num_px, 3) se aplanen en vectores simples de forma (num \\ _px $ * $ num \\ _px $ * $ 3, 1).\n",
    "\n",
    "Un truco cuando quieres aplanar una matriz X de forma (a, b, c, d) a una matriz X_plana de forma (b $ * $ c $ * $ d, a) es usar:\n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n",
    "```\n",
    "____\n",
    "### Anotaciones - Atajo del reshape\n",
    "#### **X_flatten = X.rehape (X.shape [0], -1) .T**\n",
    "1. Se define la primer dimensión\n",
    "2. La segunda dimensión (-1) significa que debe hacer la dimensión del tamaño tal que quepan todos los elementos de x\n",
    "3. El .T significa que se debe aplicar la transpuesta porque se necesita que las instancias se tengan en las columnas y no en las filas\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5nZWN6MRiq_9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n",
      "sanity check after reshaping: [17 31 56 22 33]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n------------------------------ Resultado esperado ------------------------------\\ntrain_set_x_flatten shape: (12288, 209)\\ntrain_set_y shape: (1, 209)\\ntest_set_x_flatten shape: (12288, 50)\\ntest_set_y shape: (1, 50)\\nsanity check after reshaping: [17 31 56 22 33]\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(m_test, -1).T\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0])) # Primeros 5 atributos de la fila 0\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ Resultado esperado ------------------------------\n",
    "train_set_x_flatten shape: (12288, 209)\n",
    "train_set_y shape: (1, 209)\n",
    "test_set_x_flatten shape: (12288, 50)\n",
    "test_set_y shape: (1, 50)\n",
    "sanity check after reshaping: [17 31 56 22 33]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SBGC5W5kAII"
   },
   "source": [
    "Para representar imágenes en color, se deben especificar los canales rojo, verde y azul (RGB) para cada píxel, por lo que el valor del píxel es en realidad un vector de tres números que van de 0 a 255.\n",
    "\n",
    "Un paso de preprocesamiento común en el aprendizaje automático es centrar y estandarizar su conjunto de datos, lo que significa que debe restar la media de toda la matriz numpy de cada ejemplo y luego dividir cada ejemplo por la desviación estándar de toda la matriz numpy. Pero para los conjuntos de datos de imágenes, es más simple y conveniente y funciona casi tan bien simplemente dividir cada fila del conjunto de datos por 255 (el valor máximo de un canal de píxeles).\n",
    "\n",
    "Durante el entrenamiento de su modelo, multiplicará los pesos y agregará sesgos a algunas entradas iniciales para observar las activaciones de las neuronas. Luego, se retropropone con los degradados para entrenar el modelo. Pero es extremadamente importante que cada característica tenga un rango similar para que nuestros gradientes no exploten. Verá eso con más detalle más adelante en las conferencias.\n",
    "\n",
    "Estandaricemos nuestro conjunto de datos.\n",
    "____\n",
    "### Anotaciones - Estandarización\n",
    "1. Después de definirle las dimensiones correctas a los vectores se debe hacer la normalización\n",
    "2. Antes de la normalización se debe hacer la **Estandarización**\n",
    "3. La **Estandarización** consiste en restarle la media a las filas de los atributos y luego dividirlos entre la desviación estándar\n",
    "\n",
    "* La normalización y la estandarización para mejorar el rendimiento de los modelos al entrenarlos\n",
    "* Se prefiere la estandarización porque al restar la normal los datos se centran en 0\n",
    "* La estandarización permite que al dividir entre la desviación estándar ayuda a que si hay atributos en escalas diferentes al dividirlos con la desviación estándar, quedan todos en la misma escala\n",
    "\n",
    "* Para el **tratamiento de imagenes** se utiliza un tipo de estandarización específico, solamente dividiendo los conjuntos entre 255 obteniendo los conjuntos definitivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Qk73FsA3ke1M"
   },
   "outputs": [],
   "source": [
    "# Estandarización para imagenes\n",
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TY1hTdT-k2vZ"
   },
   "source": [
    "**Lo que necesita recordar:**\n",
    "\n",
    "Los pasos comunes para preprocesar un nuevo conjunto de datos son:\n",
    "- Averiguar las dimensiones y formas del problema (m_train, m_test, num_px, ...)\n",
    "- Reformar los conjuntos de datos de modo que cada ejemplo sea ahora un vector de tamaño (num_px \\ * num_px \\ * 3, 1)\n",
    "- \"Estandarizar\" los datos\n",
    "\n",
    "\n",
    "## 3 - Arquitectura general del algoritmo de aprendizaje ##\n",
    "\n",
    "Es hora de diseñar un algoritmo simple para distinguir imágenes de gatos de imágenes que no son de gatos.\n",
    "\n",
    "Construirá una regresión logística, utilizando una mentalidad de red neuronal. La siguiente figura explica por qué **¡La regresión logística es en realidad una red neuronal muy simple!**\n",
    "\n",
    "![dZ vectorizado](https://drive.google.com/uc?export=view&id=19_hkklhmqLq61aKb0Q1_nHZ7jetBQEIq)\n",
    "\n",
    "**Expresión matemática del algoritmo**:\n",
    "\n",
    "Por ejemplo $ x ^ {(i)} $:\n",
    "$$ z ^ {(i)} = w ^ T x ^ {(i)} + b \\ etiqueta {1} $$\n",
    "$$ \\ hat {y} ^ {(i)} = a ^ {(i)} = sigmoide (z ^ {(i)}) \\ etiqueta {2} $$\n",
    "$$ \\ mathcal {L} (a ^ {(i)}, y ^ {(i)}) = - y ^ {(i)} \\ log (a ^ {(i)}) - (1-y ^ {(i)}) \\ log (1-a ^ {(i)}) \\ etiqueta {3} $$\n",
    "\n",
    "Luego, el costo se calcula sumando todos los ejemplos de capacitación:\n",
    "$$ J = \\ frac {1} {m} \\ sum_ {i = 1} ^ m \\ mathcal {L} (a ^ {(i)}, y ^ {(i)}) \\ etiqueta {6} $$\n",
    "\n",
    "**Pasos clave**:\n",
    "En este ejercicio, realizará los siguientes pasos:\n",
    "    - Inicializar los parámetros del modelo\n",
    "    - Conozca los parámetros del modelo minimizando el costo\n",
    "    - Utilice los parámetros aprendidos para hacer predicciones (en el equipo de prueba)\n",
    "    - Analizar los resultados y concluir\n",
    "\n",
    "___\n",
    "\n",
    "### Anotaciones - Arquitectura general del algoritmo de aprendizaje \n",
    "### Red neuronal de una sola neurona\n",
    "\n",
    "* La regresión logistica vista como una red neuronal es una red neuronal compuesta de solamente una neurona\n",
    "* Los elementos que la componen son: \n",
    "    1. Función de costo - A\n",
    "    2. Atributos - x\n",
    "    3. Peso - w\n",
    "* El peso coinside con cada uno de los atributos\n",
    "* Al procesarse por la función de costo, se produce una probabilidad a la que se le aplica un umbral para tomar una decisión\n",
    "_____\n",
    "\n",
    "## 4 - Construyendo las partes de nuestro algoritmo ##\n",
    "\n",
    "Los pasos principales para construir una red neuronal son:\n",
    "1. Definir la estructura del modelo (como el número de características de entrada)\n",
    "2. Inicializar los parámetros del modelo\n",
    "3. Bucle:\n",
    "     - Calcular la pérdida de corriente (propagación hacia adelante)\n",
    "     - Calcular gradiente actual (propagación hacia atrás)\n",
    "     - Actualizar parámetros (descenso de gradiente)\n",
    "\n",
    "A menudo, construye 1-3 por separado y los integra en una función que llamamos `modelo ()`.\n",
    "\n",
    "### 4.1 - Funciones de ayuda\n",
    "\n",
    "**Ejercicio**: Usando su código de \"Python Basics\", implemente `sigmoid ()`. Como ha visto en la figura anterior, necesita calcular $ sigmoidea (w ^ T x + b) = \\ frac {1} {1 + e ^ {- (w ^ T x + b)}} $ para hacer predicciones. Utilice np.exp ()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Mvqe9-QJnWfr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.5        0.88079708]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n------------------------------ Resultado esperado ------------------------------\\nsigmoid([0, 2]) = [0.5   0.88079708]\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1.0 / (1.0 + np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s\n",
    "\n",
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ Resultado esperado ------------------------------\n",
    "sigmoid([0, 2]) = [0.5   0.88079708]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MI_6__00nj0N"
   },
   "source": [
    "### 4.2 - Inicializando parámetros\n",
    "\n",
    "**Ejercicio:** Implemente la inicialización de parámetros en la celda siguiente. Tienes que inicializar w como un vector de ceros. Si no sabe qué función numpy usar, busque np.zeros () en la documentación de la biblioteca Numpy.\n",
    "\n",
    "**Se recomienda inicializarlos en 0 porque eso ayuda al aprendizaje del algoritmo**\n",
    "**Dimensiones --> dim => n**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "eKoBH0wVnqc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]]\n",
      "b = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n------------------------------ Resultado esperado ------------------------------\\nw = [[0.][0.]]\\nb = 0\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    \n",
    "    # Inicio de w y b cerca de 0\n",
    "    w = np.zeros((dim,1)) # vector columna\n",
    "    b = 0 # Valor escalar\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ Resultado esperado ------------------------------\n",
    "w = [[0.][0.]]\n",
    "b = 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV9iIBVPongK"
   },
   "source": [
    "### 4.3 - Propagación hacia adelante y hacia atrás\n",
    "\n",
    "Ahora que sus parámetros están inicializados, puede realizar los pasos de propagación \"hacia adelante\" y \"hacia atrás\" para aprender los parámetros.\n",
    "\n",
    "**Ejercicio:** Implemente una función `propagate ()` que calcule la función de costo y su gradiente.\n",
    "\n",
    "**Sugerencias**:\n",
    "\n",
    "Propagación hacia adelante:\n",
    "- Obtienes X\n",
    "- Calcula $ A = \\ sigma (w ^ TX + b) = (a ^ {(1)}, a ^ {(2)}, ..., a ^ {(m-1)}, a ^ { (m)}) $\n",
    "- Calcula la función de costo: $ J = - \\ frac {1} {m} \\ sum_ {i = 1} ^ {m} y ^ {(i)} \\ log (a ^ {(i)}) + ( 1-y ^ {(i)}) \\ log (1-a ^ {(i)}) $\n",
    "\n",
    "Aquí están las dos fórmulas que usará:\n",
    "\n",
    "$$ \\ frac {\\ parcial J} {\\ parcial w} = \\ frac {1} {m} X (A-Y) ^ T \\ etiqueta {7} $$\n",
    "$$ \\ frac {\\ parcial J} {\\ parcial b} = \\ frac {1} {m} \\ sum_ {i = 1} ^ m (a ^ {(i)} - y ^ {(i)}) \\ etiqueta {8} $$\n",
    "\n",
    "\n",
    "Aquí están las dos fórmulas que usará:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "N3eU9vAs7_a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n------------------------------ Resultado esperado ------------------------------\\ndw = [[0.99845601] [2.39507239]]\\ndb = 0.001455578136784208\\ncost = 5.801545319394553\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    PROPRAGACION PARA ADELANTE Y ATRAS\n",
    "    \n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\" \n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    Z = w.T.dot(X) + b\n",
    "    A = sigmoid(Z)                          # compute activation\n",
    "    cost = np.multiply(-(1/m),np.sum(((np.multiply(Y, np.log(A))) + (np.multiply((1-Y),(np.log(1-A)))))))# compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dZ = A - Y\n",
    "    dw = (1.0 / m) * X.dot(dZ.T)\n",
    "    db = (1.0/m) * np.sum(dZ)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape) # Coincidir las dimensiones\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost) # Eliminar dimensiones de tamaño 1 con squeeze\n",
    "    assert(cost.shape == ()) # Debe ser un valor flotante no un arreglo\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ Resultado esperado ------------------------------\n",
    "dw = [[0.99845601] [2.39507239]]\n",
    "db = 0.001455578136784208\n",
    "cost = 5.801545319394553\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZA3H9Es8Jwm"
   },
   "source": [
    "**Salida Esperada**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "    <tr>\n",
    "        <td>  dw  </td>\n",
    "      <td> [[ 0.99845601]\n",
    "     [ 2.39507239]]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  db </td>\n",
    "        <td> 0.00145557813678 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  costo  </td>\n",
    "        <td> 5.801545319394553 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "### 4.4 - Optimización del descenso de gradiente\n",
    "\n",
    "- Ha inicializado sus parámetros.\n",
    "- También puede calcular una función de costo y su gradiente.\n",
    "- Ahora, desea actualizar los parámetros usando el descenso de gradiente.\n",
    "\n",
    "** Ejercicio: ** Anote la función de optimización. El objetivo es aprender $ w $ y $ b $ minimizando la función de costo $J$. Para un parámetro $\\theta$, la regla de actualización es $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ es el ritmo de aprendizaje.\n",
    "\n",
    "___\n",
    "### Anotaciones - Función de Optimización o descenso del gradiente\n",
    "* Sirve para actualizar el costo de b con respecto a w + b\n",
    "* Teniendo la propagación hacía delante y atras sigue realizando el descenso de gradiente\n",
    "    1. Propagación hacía delante -> Costo\n",
    "    2. Propagación haćia atras -> Derivadas\n",
    "        * Sirven para actualizar paso a paso el valor de w y b\n",
    "* En la función optimizar se realiza el proceso del descenso de gradiente una cantidad determinada de veces\n",
    "* La formula de descenso de gradiente utiliza alfa para modificar la longitud del salto que se hace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yo81lx4Z88JN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.19033591]\n",
      " [0.12259159]]\n",
      "b = 1.9253598300845747\n",
      "dw = [[0.67752042]\n",
      " [1.41625495]]\n",
      "db = 0.21919450454067657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n------------------------------ Resultado esperado ------------------------------\\nw = [[0.19033591] [0.12259159]]\\nb = 1.9253598300845747\\ndw = [[0.67752042] [1.41625495]]\\ndb = 0.21919450454067657\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = [] # Registro de los costos para gráficar\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw # Vectorizado\n",
    "        b = b - learning_rate * db # Escalar\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ Resultado esperado ------------------------------\n",
    "w = [[0.19033591] [0.12259159]]\n",
    "b = 1.9253598300845747\n",
    "dw = [[0.67752042] [1.41625495]]\n",
    "db = 0.21919450454067657\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmW2Ctj99GR2"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "    <tr>\n",
    "       <td> w </td>\n",
    "       <td>[[ 0.19033591]\n",
    " [ 0.12259159]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> b </td>\n",
    "       <td> 1.92535983008 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> dw </td>\n",
    "       <td> [[ 0.67752042]\n",
    " [ 1.41625495]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> db </td>\n",
    "       <td> 0.219194504541 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "____\n",
    "** Ejercicio: ** La función anterior generará la w y b aprendidas. Podemos usar w y b para predecir las etiquetas de un conjunto de datos X. Implementar la función `predict ()`. Hay dos pasos para calcular predicciones:\n",
    "\n",
    "1. Calcule $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convierta las entradas de a en 0 (si activación <= 0.5) o 1 (si activación> 0.5), almacena las predicciones en un vector `Y_prediction`. Si lo desea, puede usar una instrucción `if` /` else` en un bucle `for` (aunque también hay una manera de vectorizar esto).\n",
    "\n",
    "_____\n",
    "### Anotaciones - Predicción\n",
    "* Para realizar las predicciones se debe hacer uso del umbral, el cual es de 0.5\n",
    "* **A** Hace referencia a la probabilidad el cual se obtiene a través de la propagación hacía delante\n",
    "* No se necesita vectorizar en operaciones que no se repeten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0oWa9rUr9hcH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n------------------------------ Resultado esperado ------------------------------\\npredictions = [[1. 1. 0.]]\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m)) # Inicializado en el mismo tamaños de m \n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(w.T.dot(X)+b)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        # Operador ternario para escribir un if en una sola línea de codigo\n",
    "        Y_prediction[0, i] = 1.0 if A[0,i] >= 0.5 else 0.0\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "\n",
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ Resultado esperado ------------------------------\n",
    "predictions = [[1. 1. 0.]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sm1Y2rQG9nvt"
   },
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "    <tr>\n",
    "         <td>\n",
    "             predicciones\n",
    "         </td>\n",
    "          <td>\n",
    "            [[ 1.  1.  0.]]\n",
    "         </td>  \n",
    "   </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "____\n",
    "**Qué recordar:**\n",
    "Ha implementado varias funciones que:\n",
    "- Inicializar (w, b)\n",
    "- Optimice la pérdida de forma iterativa para aprender los parámetros (w, b):\n",
    "     - calcular el costo y su gradiente\n",
    "     - actualización de los parámetros mediante descenso de gradiente\n",
    "- Usar lo aprendido (w, b) para predecir las etiquetas de un conjunto dado de ejemplos\n",
    "____\n",
    "## 5 - Merge all functions into a model ##\n",
    "\n",
    "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
    "\n",
    "**Exercise:** Implement the model function. Use the following notation:\n",
    "    - Y_prediction_test for your predictions on the test set\n",
    "    - Y_prediction_train for your predictions on the train set\n",
    "    - w, costs, grads for the outputs of optimize()\n",
    "    \n",
    "____\n",
    "### Anotaciones - Mezclar funciones en un solo Modelo\n",
    "1. Inicializar los parámetros en 0\n",
    "2. Realizar la optimización de los datos con el descenso del gradiente para \n",
    "    1. Parametros --> Obtener los valores definitivos de w y b\n",
    "    2. Gradientes --> Derivadas para depurar\n",
    "    3. Costos --> Verificar y graficar los costos \n",
    "3. Realizar predicciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "EgojzmY-94KL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.584508\n",
      "Cost after iteration 200: 0.466949\n",
      "Cost after iteration 300: 0.376007\n",
      "Cost after iteration 400: 0.331463\n",
      "Cost after iteration 500: 0.303273\n",
      "Cost after iteration 600: 0.279880\n",
      "Cost after iteration 700: 0.260042\n",
      "Cost after iteration 800: 0.242941\n",
      "Cost after iteration 900: 0.228004\n",
      "Cost after iteration 1000: 0.214820\n",
      "Cost after iteration 1100: 0.203078\n",
      "Cost after iteration 1200: 0.192544\n",
      "Cost after iteration 1300: 0.183033\n",
      "Cost after iteration 1400: 0.174399\n",
      "Cost after iteration 1500: 0.166521\n",
      "Cost after iteration 1600: 0.159305\n",
      "Cost after iteration 1700: 0.152667\n",
      "Cost after iteration 1800: 0.146542\n",
      "Cost after iteration 1900: 0.140872\n",
      "train accuracy: 99.04306220095694 %\n",
      "test accuracy: 70.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n------------------------------ Resultado esperado ------------------------------\\nCost after iteration 0: 0.693147\\nCost after iteration 100: 0.584508\\nCost after iteration 200: 0.466949\\nCost after iteration 300: 0.376007\\nCost after iteration 400: 0.331463\\nCost after iteration 500: 0.303273\\nCost after iteration 600: 0.279880\\nCost after iteration 700: 0.260042\\nCost after iteration 800: 0.242941\\nCost after iteration 900: 0.228004\\nCost after iteration 1000: 0.214820\\nCost after iteration 1100: 0.203078\\nCost after iteration 1200: 0.192544\\nCost after iteration 1300: 0.183033\\nCost after iteration 1400: 0.174399\\nCost after iteration 1500: 0.166521\\nCost after iteration 1600: 0.159305\\nCost after iteration 1700: 0.152667\\nCost after iteration 1800: 0.146542\\nCost after iteration 1900: 0.140872\\ntrain accuracy: 99.04306220095694 %\\ntest accuracy: 70.0 %\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w,b,X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train =  predict(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d\n",
    "\n",
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ Resultado esperado ------------------------------\n",
    "Cost after iteration 0: 0.693147\n",
    "Cost after iteration 100: 0.584508\n",
    "Cost after iteration 200: 0.466949\n",
    "Cost after iteration 300: 0.376007\n",
    "Cost after iteration 400: 0.331463\n",
    "Cost after iteration 500: 0.303273\n",
    "Cost after iteration 600: 0.279880\n",
    "Cost after iteration 700: 0.260042\n",
    "Cost after iteration 800: 0.242941\n",
    "Cost after iteration 900: 0.228004\n",
    "Cost after iteration 1000: 0.214820\n",
    "Cost after iteration 1100: 0.203078\n",
    "Cost after iteration 1200: 0.192544\n",
    "Cost after iteration 1300: 0.183033\n",
    "Cost after iteration 1400: 0.174399\n",
    "Cost after iteration 1500: 0.166521\n",
    "Cost after iteration 1600: 0.159305\n",
    "Cost after iteration 1700: 0.152667\n",
    "Cost after iteration 1800: 0.146542\n",
    "Cost after iteration 1900: 0.140872\n",
    "train accuracy: 99.04306220095694 %\n",
    "test accuracy: 70.0 %\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buu-J7TM-W1F"
   },
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "    <tr>\n",
    "        <td> Cost after iteration 0   </td> \n",
    "        <td> 0.693147 </td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "        <td> <center> $\\vdots$ </center> </td> \n",
    "        <td> <center> $\\vdots$ </center> </td> \n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <td> Train Accuracy  </td> \n",
    "        <td> 99.04306220095694 % </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Test Accuracy </td> \n",
    "        <td> 70.0 % </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "____\n",
    "\n",
    "**Comentario**: La precisión del entrenamiento es cercana al 100%. Esta es una buena verificación de cordura: su modelo está funcionando y tiene una capacidad lo suficientemente alta para ajustarse a los datos de entrenamiento. La precisión de la prueba es del 68%. En realidad, no es malo para este modelo simple, dado el pequeño conjunto de datos que usamos y que la regresión logística es un clasificador lineal. Pero no se preocupe, ¡creará un clasificador aún mejor la próxima semana!\n",
    "\n",
    "Además, verá que el modelo claramente sobreajusta los datos de entrenamiento. Más adelante en esta especialización, aprenderá a reducir el sobreajuste, por ejemplo, mediante la regularización. Usando el siguiente código (y cambiando la variable `index`) puede ver las predicciones en las imágenes del conjunto de prueba.\n",
    "\n",
    "### Anotaciones - Overfiting o Sobreajuste\n",
    "* Para mejorar la forma de generalizar del modelo se puede hacer uso de una cantidad mayor de datos o crear redes neurales más grande\n",
    "* El ** Overfiting o Sobreajuste ** Se da cuando el modelo se apega a los datos de entrenamiento y no es capaz de generalizar con los datos de prueba\n",
    "    1. Para tratar el Sobreajuste se usa la regularización\n",
    "    2. Si se realizan más iteraciones en el descenso del gradiente se puden obtener costos más bajos, pero eso implica una mayor precisión de entrenamiento pero no necesariamente en los datos de prueba y podría inducir a un overfiting o sobreajuste mayor\n",
    "    \n",
    "    \n",
    "### Regularización\n",
    "\n",
    "[Técnicas de Regularización Básicas para Redes Neuronales] (https://medium.com/metadatos/t%C3%A9cnicas-de-regularizaci%C3%B3n-b%C3%A1sicas-para-redes-neuronales-b48f396924d4)\n",
    "\n",
    "[Una aproximación a la regularización de redes cascada-correlación para la predicción de series de tiempo] aproximacion-a-la-regularizacion-de-redes-cascada-correlacion-para-la-prediccion-de-series-de-tiempo.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "XfFBheaZ-qzT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1, you predicted that it is a \"cat\" picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture that was wrongly classified.\n",
    "index = 3\n",
    "plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\n",
    "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(d[\"Y_prediction_test\"][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ Resultado esperado ------------------------------\n",
    "y = 1, you predicted that it is a \"non-cat\" picture.\n",
    "\"\"\"\n",
    "\n",
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPVG88jG-80D"
   },
   "source": [
    "**Interpretación**:\n",
    "Puede ver que el costo disminuye. Muestra que se están aprendiendo los parámetros. Sin embargo, verá que podría entrenar el modelo aún más en el conjunto de entrenamiento. Intente aumentar el número de iteraciones en la celda de arriba y vuelva a ejecutar las celdas. Es posible que vea que la precisión del conjunto de entrenamiento aumenta, pero la precisión del conjunto de prueba disminuye. A esto se le llama sobreajuste.\n",
    "\n",
    "![learning rate](https://drive.google.com/uc?export=view&id=1IvCcu7w--BYTfSMWMqtaFW01WitsXWT8)\n",
    "\n",
    "_____\n",
    "## 6 - Análisis adicional (ejercicio opcional / sin calificar) ##\n",
    "\n",
    "Felicitaciones por crear su primer modelo de clasificación de imágenes. Analicémoslo más a fondo y examinemos las posibles opciones para la tasa de aprendizaje $\\alpha$.\n",
    "\n",
    "#### Elección de la tasa de aprendizaje ####\n",
    "\n",
    "**Recordatorio**:\n",
    "Para que Gradient Descent funcione, debe elegir la velocidad de aprendizaje con prudencia. La tasa de aprendizaje $ \\ alpha $ determina la rapidez con la que actualizamos los parámetros. Si la tasa de aprendizaje es demasiado grande, podemos \"sobrepasar\" el valor óptimo. Del mismo modo, si es demasiado pequeño, necesitaremos demasiadas iteraciones para converger a los mejores valores. Por eso es fundamental utilizar una tasa de aprendizaje bien ajustada.\n",
    "\n",
    "Comparemos la curva de aprendizaje de nuestro modelo con varias opciones de tasas de aprendizaje. Ejecute la celda a continuación. Esto debería tomar alrededor de 1 minuto. Siéntase libre de probar valores diferentes a los tres que hemos inicializado para contener la variable `learning_rates` y ver qué sucede.\n",
    "\n",
    "____\n",
    "### Anotaciones - Determinar un learning rate o ritmo de aprendizaje\n",
    "* Se hace uso de prueba y error utilizando posibles ritmos de aprendizaje para determinar cual es el ritmo de aprendizaje que consigue mejores resultados se hace un entrenamiento usando los ritmos propuestos y luego se grafican\n",
    "* El costo está relacionado con la presición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zbrAh38hArmX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is: 0.01\n",
      "train accuracy: 99.52153110047847 %\n",
      "test accuracy: 68.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "learning rate is: 0.001\n",
      "train accuracy: 88.99521531100478 %\n",
      "test accuracy: 64.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "learning rate is: 0.0001\n",
      "train accuracy: 68.42105263157895 %\n",
      "test accuracy: 36.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n------------------------------ Resultado esperado ------------------------------\\n\\nlearning rate is: 0.01\\ntrain accuracy: 99.52153110047847 %\\ntest accuracy: 68.0 %\\n\\n-------------------------------------------------------\\n\\nlearning rate is: 0.001\\ntrain accuracy: 88.99521531100478 %\\ntest accuracy: 64.0 %\\n\\n-------------------------------------------------------\\n\\nlearning rate is: 0.0001\\ntrain accuracy: 68.42105263157895 %\\ntest accuracy: 36.0 %\\n\\n-------------------------------------------------------\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "------------------------------ Resultado esperado ------------------------------\n",
    "\n",
    "learning rate is: 0.01\n",
    "train accuracy: 99.52153110047847 %\n",
    "test accuracy: 68.0 %\n",
    "\n",
    "-------------------------------------------------------\n",
    "\n",
    "learning rate is: 0.001\n",
    "train accuracy: 88.99521531100478 %\n",
    "test accuracy: 64.0 %\n",
    "\n",
    "-------------------------------------------------------\n",
    "\n",
    "learning rate is: 0.0001\n",
    "train accuracy: 68.42105263157895 %\n",
    "test accuracy: 36.0 %\n",
    "\n",
    "-------------------------------------------------------\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d13f-o9GAx6O"
   },
   "source": [
    "\n",
    "![dZ vectorizado](https://drive.google.com/uc?export=view&id=1IsuERqIjX8XMR6D9dd73HdDACuM84kZL)\n",
    "____\n",
    "**Interpretación**:\n",
    "- Diferentes tasas de aprendizaje dan diferentes costos y, por lo tanto, diferentes resultados de predicciones.\n",
    "- Si la tasa de aprendizaje es demasiado grande (0.01), el costo puede oscilar hacia arriba y hacia abajo. Incluso puede divergir (aunque en este ejemplo, el uso de 0.01 eventualmente termina teniendo un buen valor por el costo).\n",
    "- Un menor costo no significa un mejor modelo. Tienes que comprobar si existe un sobreajuste. Ocurre cuando la precisión del entrenamiento es mucho mayor que la precisión de la prueba.\n",
    "- En el aprendizaje profundo, normalmente le recomendamos que:\n",
    "     - Elija la tasa de aprendizaje que minimice mejor la función de costo.\n",
    "     - Si su modelo se sobreajusta, utilice otras técnicas para reducir el sobreajuste. (Hablaremos de esto en videos posteriores).\n",
    "____\n",
    "## 7 - Prueba con tu propia imagen (ejercicio opcional / sin calificar) ##\n",
    "\n",
    "Felicitaciones por terminar esta tarea. Puede usar su propia imagen y ver el resultado de su modelo. Para hacer eso:\n",
    "     1. Haga clic en \"Archivo\" en la barra superior de este cuaderno, luego haga clic en \"Abrir\" para ir a su Coursera Hub.\n",
    "     2. Agregue su imagen al directorio de este Jupyter Notebook, en la carpeta \"imágenes\"\n",
    "     3. Cambie el nombre de su imagen en el siguiente código\n",
    "     4. Ejecute el código y compruebe si el algoritmo es correcto (1 = gato, 0 = no gato).\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ClAGIO5pBRgH"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.ndimage' has no attribute 'imread'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-38cf3dc30572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# We preprocess the image to fit your algorithm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"images/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmy_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmy_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_px\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_px\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_px\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_px\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy.ndimage' has no attribute 'imread'"
     ]
    }
   ],
   "source": [
    "## START CODE HERE ## (PUT YOUR IMAGE NAME) \n",
    "my_image = \"my_image.jpg\"   # change this to the name of your image file \n",
    "## END CODE HERE ##\n",
    "\n",
    "# We preprocess the image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "image = image/255.\n",
    "my_image = ndimage.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\n",
    "my_predicted_image = predict(d[\"w\"], d[\"b\"], my_image)\n",
    "\n",
    "plt.imshow(image)\n",
    "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cK3J4imJBVOP"
   },
   "source": [
    "\n",
    "**Qué recordar de esta tarea:**\n",
    "1. El preprocesamiento del conjunto de datos es importante.\n",
    "2. Implementó cada función por separado: inicializar (), propagar (), optimizar (). Luego construiste un modelo ().\n",
    "3. Ajustar la tasa de aprendizaje (que es un ejemplo de un \"hiperparámetro\") puede marcar una gran diferencia en el algoritmo. ¡Verás más ejemplos de esto más adelante en este curso!\n",
    "\n",
    "____\n",
    "Finalmente, si lo desea, lo invitamos a probar diferentes cosas en este Cuaderno. Asegúrate de enviarlo antes de intentar cualquier cosa. Una vez que envíe, las cosas con las que puede jugar incluyen:\n",
    "     - Juega con la tasa de aprendizaje y el número de iteraciones.\n",
    "     - Pruebe diferentes métodos de inicialización y compare los resultados\n",
    "     - Pruebe otros preprocesos (centre los datos o divida cada fila por su desviación estándar)\n",
    "____\n",
    "  \n",
    "Bibliografia:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Clase_0301-0304.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
